# 验证与测试方案

> 本文档定义 AI Context 框架的验证方法和测试标准。

---

## 功能验证清单

### 文件结构验证

```markdown
## 初始化验证清单

### 文件结构验证
- [ ] docs/AI_CONTEXT/_RULES.md 存在且内容完整
- [ ] docs/AI_CONTEXT/MAP.md 存在且包含基本结构
- [ ] 至少一个模块有 _AI_CONTEXT.md
- [ ] 所有文件使用 UTF-8 编码

### Git Hook 验证
- [ ] .git/hooks/pre-commit 存在
- [ ] .git/hooks/pre-commit 有执行权限
- [ ] 执行测试提交时 hook 正常触发

### 内容验证
- [ ] MAP.md [项目概览] 描述准确
- [ ] MAP.md [模块职责] 覆盖主要目录
- [ ] _AI_CONTEXT.md 职责描述与代码一致
```

---

## AI 理解测试

创建测试提示词，验证 AI 是否能正确理解项目：

### 测试提示词模板

```markdown
## AI 理解测试提示词

请阅读 docs/AI_CONTEXT/ 目录下的文档，然后回答以下问题：

### 基础理解测试
1. 用一句话描述这个项目是做什么的？
2. 项目有几个主要模块？分别叫什么？
3. 如果我想修改登录功能，应该看哪个目录的代码？

### 深度理解测试
4. 描述一下用户登录的完整执行流程
5. [核心概念A] 和 [核心概念B] 是什么关系？
6. 如果我要添加一个新的 API 接口，应该遵循什么约定？

### 实践测试
7. 请帮我在 [模块X] 中添加一个 [功能Y]

---

预期结果：
- 问题 1-3：AI 应该能准确回答，无需查看代码
- 问题 4-6：AI 应该能基本准确回答，可能需要查看少量代码验证
- 问题 7：AI 应该能找到正确的位置，遵循项目约定实现
```

### 测试执行方法

1. **开启新对话** - 模拟 AI 首次接触项目
2. **执行测试提示词** - 按顺序提问
3. **记录回答准确度** - 标记正确/部分正确/错误
4. **分析改进点** - 识别文档不足之处

---

## 验证成功标准

| 指标 | 成功标准 | 验证方法 |
|------|----------|----------|
| 文档完整性 | 核心文件全部存在 | 检查文件是否存在 |
| 内容准确性 | 关键信息准确率 > 90% | 人工抽检 |
| AI 理解度 | 基础问题正确率 100% | AI 测试提示词 |
| Hook 可用性 | 提交时正常触发 | 测试提交 |

### 分级评估

#### Level A - 优秀
- 所有核心文件存在且内容完整
- AI 能在 2 分钟内理解项目整体
- AI 能准确定位任意功能的实现位置

#### Level B - 良好
- 核心文件存在，部分内容待完善
- AI 能在 5 分钟内理解项目整体
- AI 能定位大部分功能的实现位置

#### Level C - 及格
- 基本文件存在
- AI 需要阅读代码才能理解项目
- 文档起到辅助作用

---

## 质量保证指标

### 文档质量指标

#### 完整性
- [ ] `_RULES.md` 包含完整的 AI 行为协议
- [ ] `MAP.md` 所有章节都有实质内容
- [ ] 所有重要模块都有 `_AI_CONTEXT.md`

#### 准确性
- [ ] 模块职责描述与代码一致
- [ ] 关键路径的执行流程与代码一致
- [ ] 关键接口的签名与代码一致

#### 可用性
- [ ] 新 AI 对话能在 5 分钟内理解项目
- [ ] AI 能正确回答"功能 X 在哪实现？"
- [ ] AI 能正确判断"新功能 Y 应该加在哪？"

### 技术约束

#### 开发约束
- **Python 版本**: 3.6+（仅使用标准库）
- **跨平台**: macOS/Linux/Windows
- **性能**: Git hook 执行应在 5 秒内完成

#### 文档约束
- **格式**: Markdown
- **大小**: MAP.md < 20KB，_AI_CONTEXT.md < 10KB
- **命名**: `_AI_CONTEXT.md`（下划线前缀，便于排序）

#### Git 约束
- **非侵入**: 文档更新失败不应阻断 commit
- **原子性**: 文档更新应与代码变更在同一 commit
- **可选**: 可通过配置禁用自动更新

---

## 常见问题解答

### Q: 初始化需要多长时间？

| 项目规模 | 预计时间 |
|----------|----------|
| 小型项目（< 10 个文件） | 30 分钟 |
| 中型项目（10-50 个文件） | 1-2 小时 |
| 大型项目（> 50 个文件） | 2-4 小时 |

### Q: 哪些目录需要创建 _AI_CONTEXT.md？

**需要创建**:
- 核心业务逻辑目录
- 被多个模块依赖的基础设施目录
- 包含复杂逻辑的目录
- 有特殊约定的目录

**不需要创建**:
- 纯配置文件目录
- 静态资源目录
- 测试数据目录
- 第三方代码目录

### Q: 如何处理已有的 README.md？

两种策略：
1. **共存**：README 面向人类，_AI_CONTEXT.md 面向 AI，内容可以有重叠
2. **引用**：在 _AI_CONTEXT.md 中引用 README 的某些章节，避免重复

### Q: 多人协作时如何避免冲突？

1. 使用 AUTO_SYNC 标记让系统自动管理
2. MANUAL 区域的修改应该先沟通
3. 将 _AI_CONTEXT.md 纳入 Code Review

### Q: 文档过时了怎么办？

1. **自动检测**：doc-maintainer 会在每次提交时检查
2. **手动触发**：运行 `python3 scripts/doc_maintainer.py --full-check`
3. **定期审查**：建议每周花 10 分钟检查核心模块文档

### Q: 如何验证文档是否有效？

1. **新人测试**：让新成员通过 AI 理解项目，记录困难点
2. **AI 测试**：使用本文档的测试提示词验证
3. **代码审查**：在 CR 中检查文档是否与代码同步更新

---

## 持续改进

### 反馈收集

```markdown
## 文档反馈模板

### 发现的问题
- 文件: [文件路径]
- 章节: [章节名]
- 问题描述: [具体问题]
- 建议修改: [修改建议]

### 缺失的内容
- 缺失内容描述: [描述]
- 建议添加位置: [位置]
- 优先级: [高/中/低]
```

### 改进周期

1. **每周**：检查核心模块文档同步
2. **每月**：执行一次完整的 AI 理解测试
3. **每季度**：审查整体文档架构，考虑优化

---

## 相关文档

- [Git Hook 机制](./001-Git-Hook机制.md)
- [doc-maintainer 设计](./002-doc-maintainer设计.md)
- [增量更新策略](./003-增量更新策略.md)
